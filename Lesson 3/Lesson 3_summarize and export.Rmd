---
title: "L3 plan"
author: "Geoffrey Millard"
date: "9/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goals
  - Get my data out! (exporting, `write.csv()`, or use an RMarkdown)
  - Explore dates (`lubridate`)
  - Summarize data (`dplyr` from `tidyverse`)
  - Generate custom categories
  - Switch between data frame formats (long v. wide)
  - Linear regression and correlation (`lm()`, `cor()`)
  - Shapiro-Wilks, normality test (personally, I failed this test)
  - ANOVA
  - Tukey Honestly Significant Difference
  Time allowing
  - ANOVA contrasts

## Best Practice

Always start your code by cleaning out your environment.

```{r}
rm(list = ls(all.names = TRUE))
```



## Needed Packages

If we cover everything these are the packages we will need.  I'll introduce them when we start using each package.

```{r}
library(dataRetrieval)
library(tidyverse)
library(tidyselect)
library(lubridate) # Part of tidyverse
library(kableExtra)
library(broom)
# time allowing
library(multcomp)
```

## Where we left off

We were importing actual data from NWIS and cleaning it up for today.

```{r importing}
#USGS and EPA dataset retrieval package
library(dataRetrieval) 
#click on "Packages" tab to see functions available in the dataRetrival package

#these are the USGS sites upstream from Honnedaga Lake, NY that we found in the browser
sites <- c('0134277112', '0134277114')

#detailed location information about the site
# readNWISsite(c('0134277112', '0134277114'))

#whatNWISdata displays the datasets available at each site
available <- whatNWISdata(siteNumber=c('0134277112', '0134277114'))
actual <- readNWISpCode(parameterCd = available$parm_cd) #interpret parameter codes
want <- c('00681', '00945', '50287', '50285', '00403', '00409') #we want: DOC, DSO4, DHg and DMeHg
data <- readNWISqw(siteNumbers = sites, parameterCd=want) #get the data
codes <- readNWISpCode(parameterCd = unique(data$parm_cd)) #confirms that we got what we wanted, shows UNITS!
# View(data)
```


```{r parameter names}
data <- data %>% mutate(Analyte = recode(parm_cd, 
                         '00403'  = 'pH',
                         '00409'  = 'Alkalinity',
                         '00681' = "DOC", 
                         '00945' = "SO4", 
                         '50287' = "Hg", 
                         '50285' = "MeHg"))

data <- data %>% mutate(Site = recode(site_no, 
                         '0134277112' = "Reference", 
                         '0134277114' = "Treated"))
colnames(data)
data1 <- data %>% dplyr::select(Site, Date=sample_dt, Time=sample_tm, Analyte, Result=result_va)

data1$Treatment <- NA
data1$Treatment[ data1$Date < ymd('2013-10-1')  ] <- 1
data1$Treatment[ data1$Date >= ymd('2013-10-1') ] <- 2
data1$Treatment[ data1$Date > mdy('2014-02-28') ] <- 3  # error because I am using the wrong lubridate function
```


```{r corrected Treatment variable}
data1$Treatment[ data1$Date > ymd('2014-02-28') ] <- 3
```


It would also be nice to set this as a factor or categorical variable.  This will be useful if you want your categorical data to always display in the same order when plotting or summarizing.

```{r factor Treatment}
data1$Treatment <- factor(data1$Treatment, levels = c(1, 2, 3), labels = c('Pre-Treatment', 'Transitional', 'Post-Treatment'), ordered = T)
head(data1$Treatment)
```


## Checking out already?

Maybe this is all you needed to do and now you're sharing it with a colleague who is going to do everything else (wouldn't that be lovely!).  If you are working in an Rmarkdown, the html output copies into MS Office applications very nicely, or you can output to a word document.  

If you want something systematic (eg. because your doing this with lots of datasets) the baseR `write.csv` function is pretty useful.  You do need to specify the dataframe and the filename and it will appear in your working directory.

```{r}
# write.csv(data1, 'Simplified_long_format.csv')
```

Maybe that colleague isn't happy with the long format data (analyte and result column).  Maybe they want a wide format where each analyte has it's own column.  What a pain in excel, but fear not!  `tidyverse` is here

```{r}
data_wide <- data1 %>% pivot_wider(names_from = Analyte, values_from = Result)

data1[,-3] %>% pivot_wider(names_from = Analyte, values_from = Result)  # example issue, remove the time column

View(data_wide)
```

There are some things that work better with long data, so we can go the other way as well with `tidyverse`

## Exercise Break 1

Can you use data_wide and the `pivot_longer()` function to generate a long format df?

```{r Ex break 1, echo=FALSE}
data_wide %>% pivot_longer(colnames(data_wide)[5:8], names_to='Name', values_to='Value')
```


## That thing everybody loves: dating

The lubridate package is a huge improvement over managing dates with baseR (eg. baseR months go from 0-11, so don't get tripped up!), because it makes it easier to separate days, months and years.  

First, Lets check and see if our Date data is actually a date.

```{r check data class}
str(data1$Date) # can also use this on the whole data frame, but that can be hard to follow on a larger dataset

class(data1$Date)
```

## Look at these: Summary stats

Now that we have cleaned data, we want to take a quick look at it.  The fastest, simplest method is called 'piping' (`%>%`) and is part of the `dplyr` package.  We can use this to generate an overall average for each site, analyte or any other variable with a column in the data frame.

```{r summary table}
Table1 <- data1[data1$Analyte=='DOC',] %>%
  group_by(Site, Analyte, year(Date)) %>% 
  summarize(average=mean(Result), n=n())
Table1
```

This is wonderful, but does not actually look like a publishable table.  `kableExtra` to save the day!

```{r kable summary table}
kable(Table1) %>% 
  kable_styling(bootstrap_options = c('striped', 'hover'), full_width = F)
```


# Exercise break 2

For each site, analyte, and month, generate the mean, standard deviation, maximum and minimum values, and the n-value?


```{r Solution Ex. 2, echo=FALSE}
Table2 <- data1 %>% 
  group_by(Site, Analyte, month(Date)) %>% 
  summarize(average=mean(Result, na.rm=T), 
            stdv=sd(Result, na.rm=T),
            max=max(Result, na.rm=T), 
            min=min(Result, na.rm=T), 
            n = n())
Table2
Table2 %>% kable() %>% 
kable_styling(bootstrap_options = c('striped', 'hover'), full_width = F)
```


## Getting at the treatment

Maybe we want to look at the three treatment periods (novel idea).

```{r}
Table5 <- data1 %>% 
  group_by(Treatment, Analyte, Site) %>% 
  summarize(average=mean(Result, na.rm=T), 
            stdv=sd(Result, na.rm=T),
            max=max(Result, na.rm=T), 
            min=min(Result, na.rm=T), 
            n = n())
Table5
Table5 %>% kable() %>% 
kable_styling(bootstrap_options = c('striped', 'hover'), full_width = F)
```

## Basic statistics

Linear regressions and correlations are common test, and we can use this data to explore how baseR presents these results.  The `lm()` function uses a formula syntax, while `cor()` does not.

```{r linear regression}
colnames(data_wide)
reg1 <- lm(data=data_wide, Hg~DOC)
reg1

summary(reg1)
plot(reg1)
```

## Exercise Break 3

Can you use the `cor()` function to generate a correlation between DOC and MeHg?

```{r correlation, echo=FALSE}
cor(x=data_wide$DOC, y=data_wide$MeHg, use = 'complete.obs')
```

## Normality and ANOVA

If you want to check that your data is normally distributed, you can use the `shapiro.test()` function.  This performs a Shapiro-Wilks test where the null hypothesis states the data is normally distributed

```{r}
shapiro.test(data_wide$Alkalinity)
```

Yay! This data is normal!  We can use it for ANOVA tests! We need to start by making an `aov()` object. This also uses the formula format.

```{r}
anova1 <- aov(data = data_wide, DOC~Treatment+Site+Treatment*Site)
summary(anova1)
```

We can use this `aov()` object to perform a Tukey HSD test and generate a rough contrast plot.

```{r, results=F}
test <- TukeyHSD(anova1)
plot(test)
```


```{r, fig.height=10, fig.width=10}
par(mar = c(4.1, 20.1, 2.1, 2.1))
plot(test, las=1)
```

## But I can write my own contrasts in SAS!

And you can do that here too, but it requires a little more work than the Tukey HSD.  We need the `multcomp` package an `interaction()` call and a new `aov()` object.

```{r}
library(multcomp)
data_wide$TMT <- interaction(data_wide$Site, data_wide$Treatment)
aov2 <- aov(data=data_wide, DOC ~ TMT)
summary(aov2)
```

Now we can write contrasts:

```{r}
cntrMat <- rbind(
  "Main effect of Site"=c(1, 1, 1, -1, -1, -1),
  "(ref1-treat1)-(ref3-treat3)"=c(1, 0, -1, -1, 0, 1),
  "(ref1-treat1)-(ref2-treat2)"=c(1,  -1, 0, -1, 1, 0), 
  "(ref2-treat2)-(ref3-treat3)"=c(0, 1, -1, 0, -1, 1)
  ) 
```

The contrasts above could also be labeled as differences between time periods (eg. (ref1-ref3)-(treat1-treat3)).  Mathematically, these are the same.


```{r}
glht(aov2, linfct=mcp(TMT=cntrMat), alternative="two.sided") # alternative='two.sided' is the default
```

This gets a little complicated to understand everything that is happening in this single line of code.  The `glht()` tests if the mean is different from zero by default.  The `linfct=mcp(TMT=cntrMat)` changes this default test to look for significant differences between the groups specified in `cntrMat`, which is what we want.