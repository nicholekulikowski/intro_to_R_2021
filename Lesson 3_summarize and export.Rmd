---
title: "L3 plan"
author: "Geoffrey Millard"
date: "9/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Best Practice

Always start your code by cleaning out your environment.

```{r}
rm(list = ls(all.names = TRUE))
```



## Needed Packages

If we cover everything these are the packages we will need.  I'll introduce them when we start using each package.

```{r}
library(dataRetrieval)
library(tidyverse)
library(lubridate) # Part of tidyverse
library(multcomp)
library(kableExtra)
library(broom)
```

## Where we left off

We were importing actual data from NWIS and cleaning it up for today.

```{r importing}
#USGS and EPA dataset retrieval package
library(dataRetrieval) 
#click on "Packages" tab to see functions available in the dataRetrival package

#these are the USGS sites upstream from Honnedaga Lake, NY that we found in the browser
sites <- c('0134277112', '0134277114')

#detailed location information about the site
# readNWISsite(c('0134277112', '0134277114'))

#whatNWISdata displays the datasets available at each site
available <- whatNWISdata(siteNumber=c('0134277112', '0134277114'))
actual <- readNWISpCode(parameterCd = available$parm_cd) #interpret parameter codes
want <- c('00681', '00945', '50287', '50285') #we want: DOC, DSO4, DHg and DMeHg
data <- readNWISqw(siteNumbers = sites, parameterCd=want) #get the data
codes <- readNWISpCode(parameterCd = unique(data$parm_cd)) #confirms that we got what we wanted, shows UNITS!
View(data)
```


```{r parameter names}
data <- data %>% mutate(Analyte = recode(parm_cd, 
                         '00403'  = 'pH',
                         '00409'  = 'Alkalinity',
                         '00681' = "DOC", 
                         '00945' = "SO4", 
                         '50287' = "Hg", 
                         '50285' = "MeHg"))

data <- data %>% mutate(Site = recode(site_no, 
                         '0134277112' = "Reference", 
                         '0134277114' = "Treated"))
```


## Look at these: Summary stats

Now that we have cleaned data, we want to take a quick look at it.  The fastest, simplest method is called 'piping' (`%>%`) and is part of the `dplyr` package.  We can use this to generate an overall average for each site and analyte.

```{r}
data %>% group_by(Site, Analyte) %>% summarize(average=mean(result_va, na.rm = T))
```

# Exercise break 1

Can you generate the standard deviation, maximum and minimum values, and the n-value?


```{r}
data %>% 
  group_by(Site, Analyte) %>% 
  summarize(average=mean(result_va), 
            stdv=sd(result_va, na.rm=T),
            max=max(result_va, na.rm=T), 
            min=min(result_va, na.rm=T), 
            n = n())
```

While this is fun, I can think of no application where we would actually want to average a whole dataset.  We might want to include a year, or month?

## That thing everybody hates: dating

The lubridate package is a huge improvement over managing dates with baseR (eg. The months do go from 0-11, so don't get tripped up!), because it makes it easier to separate days, months and years.  

We want to use dates to separate out the pre-treatment, transitional, and post-treatment time periods in this dataset.




```{r}
data$Treatment <- NA
data$Treatment[ data$sample_dt < as.Date('2013-10-1')  ] <- 1
data$Treatment[ data$sample_dt >= as.Date('2013-10-1') ] <- 2
data$Treatment[ data$sample_dt > as.Date('2014-02-28') ] <- 3

data <- data %>% mutate(Treatment2 = if(), 
                         '00403'  = 'pH',
                         '00409'  = 'Alkalinity',
                         '00681' = "DOC", 
                         '00945' = "SO4", 
                         '50287' = "Hg", 
                         '50285' = "MeHg"))

```

